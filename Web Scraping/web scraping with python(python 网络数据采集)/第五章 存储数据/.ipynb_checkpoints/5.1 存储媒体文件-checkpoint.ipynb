{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 存储媒体文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 存储媒体文件的两种主要方式：只获取文件URL链接，或者直接把源文件下载下来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以通过媒体文件所在的URL链接直接引用它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点\n",
    "- 爬虫运行更快，耗费的流量更少，只需要链接，不需要下载文件\n",
    "- 可以节省存储空间，只需要存储URL链接\n",
    "- 存储URL的代码更容易写，也不需要实现文件下载代码\n",
    "- 不下载文件可以降低目标主机服务器的负载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺点\n",
    "- 内嵌在你的网站或者应用中的外站URL链接被称为盗链（hotlinking）,使用盗链可能会有麻烦，每个网站都有防盗链的措施。\n",
    "- 链接文件在别人的服务器上，所以你的应用就要跟着别人的节奏运行\n",
    "- 盗链很容易改变\n",
    "- 现实中的浏览器不仅可以请求HTML页面并切换页面，也会下载访问页面上所有的资源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib.request.urlretrieve可以根据文件的URL下载文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T06:38:15.662315Z",
     "start_time": "2018-03-26T06:38:13.295542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('logo.jpg', <http.client.HTTPMessage at 0x250f358e3c8>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "imageLocation = soup.find(\"a\", {\"id\":\"logo\"}).find(\"img\")[\"src\"]\n",
    "urlretrieve(imageLocation, \"logo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T06:54:10.933733Z",
     "start_time": "2018-03-26T06:54:09.232130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('book.jpg', <http.client.HTTPMessage at 0x250f35b1a20>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageLocation = soup.find(\"a\", {\"target\":\"_blank\"}).find(\"img\")[\"src\"]\n",
    "urlretrieve(imageLocation, \"book.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数爬虫都不可能一天只下载一个文件。下面的程序会把http://pythonscraping.com 主页上所有src属性的文件都下载下来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载该主页上的所有SRC属性的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T07:36:17.442341Z",
     "start_time": "2018-03-26T07:36:17.426267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "downloadDirectory = \"downloaded\"\n",
    "baseUrl = \"http://pythonscraping.com\"\n",
    "def getAbsoluteURL(baseUrl, source):\n",
    "    if source.startswith(\"http://www.\"):\n",
    "        url = \"http://\" + source[11:]\n",
    "    elif source.startswith(\"http://\"):\n",
    "        url = source\n",
    "    elif source.startswith(\"www.\"):\n",
    "        url = source[4:]\n",
    "        url = \"http://\" + source\n",
    "    else:\n",
    "        url = baseUrl + \"/\" + source\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "def  getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace(\"www.\", \"\")\n",
    "    path = path.replace(baseUrl, \"\")\n",
    "    path = downloadDirectory + path\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T07:36:21.766316Z",
     "start_time": "2018-03-26T07:36:17.786816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://pythonscraping.com/misc/jquery.js?v=1.4.4\n",
      "http://pythonscraping.com/misc/jquery.js?v=1.4.4 cannot download!\n",
      "http://pythonscraping.com/misc/jquery.once.js?v=1.2\n",
      "http://pythonscraping.com/misc/jquery.once.js?v=1.2 cannot download!\n",
      "http://pythonscraping.com/misc/drupal.js?os2esm\n",
      "http://pythonscraping.com/misc/drupal.js?os2esm cannot download!\n",
      "http://pythonscraping.com/sites/all/themes/skeletontheme/js/jquery.mobilemenu.js?os2esm\n",
      "http://pythonscraping.com/sites/all/themes/skeletontheme/js/jquery.mobilemenu.js?os2esm cannot download!\n",
      "http://pythonscraping.com/sites/all/modules/google_analytics/googleanalytics.js?os2esm\n",
      "http://pythonscraping.com/sites/all/modules/google_analytics/googleanalytics.js?os2esm cannot download!\n",
      "http://pythonscraping.com/sites/default/files/lrg_0.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-be07558945bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetDownloadPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownloadDirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0a30581d5a50>\u001b[0m in \u001b[0;36mgetDownloadPath\u001b[1;34m(baseUrl, absoluteUrl, downloadDirectory)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m  \u001b[0mgetDownloadPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsoluteUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownloadDirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabsoluteUrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"www.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-be07558945bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetDownloadPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileUrl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownloadDirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileUrl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" cannot download!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "downloadList = soup.findAll(src=True)\n",
    "\n",
    "for download in downloadList:\n",
    "    fileUrl = getAbsoluteURL(baseUrl, download[\"src\"])\n",
    "    if fileUrl is not None:\n",
    "        print(fileUrl)\n",
    "    try:\n",
    "        urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))\n",
    "    except Exception as e:\n",
    "        print(fileUrl + \" cannot download!\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
